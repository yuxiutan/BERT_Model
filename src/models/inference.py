"""
æ¨¡å‹æ¨è«–æ¨¡çµ„
"""

import json
import pickle
import torch
import numpy as np
import requests
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from transformers import BertForSequenceClassification, BertTokenizer, BertModel

from ..utils.config import settings
from ..utils.logger import setup_logger, api_logger

logger = setup_logger(__name__)

class ModelInference:
    """æ¨¡å‹æ¨è«–å™¨"""
    
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.bert_model = None
        self.reference_embeddings = None
        self._initialized = False
    
    async def initialize(self):
        """åˆå§‹åŒ–æ¨¡å‹å’Œç›¸é—œçµ„ä»¶"""
        try:
            logger.info("æ­£åœ¨åˆå§‹åŒ–æ¨¡å‹æ¨è«–å™¨...")
            
            # è¼‰å…¥tokenizerå’ŒBERTæ¨¡å‹
            self.tokenizer = BertTokenizer.from_pretrained(settings.MODEL_NAME)
            self.bert_model = BertModel.from_pretrained(settings.MODEL_NAME)
            self.bert_model.eval()
            
            # æª¢æŸ¥æ˜¯å¦å­˜åœ¨é è¨“ç·´æ¨¡å‹
            model_dir = settings.MODEL_DIR
            if (model_dir / "config.json").exists():
                logger.info("ç™¼ç¾ç¾æœ‰æ¨¡å‹ï¼Œæ­£åœ¨è¼‰å…¥...")
                # è¼‰å…¥ç¾æœ‰çš„åˆ†é¡æ¨¡å‹
                self.model = BertForSequenceClassification.from_pretrained(str(model_dir))
                self.model.eval()
                logger.info("ç¾æœ‰æ¨¡å‹è¼‰å…¥æˆåŠŸ")
            else:
                logger.info("æœªç™¼ç¾ç¾æœ‰æ¨¡å‹ï¼Œå°‡ä½¿ç”¨é è¨­BERTé€²è¡Œæ¨è«–")
                # å¦‚æœæ²’æœ‰é è¨“ç·´æ¨¡å‹ï¼Œåˆå§‹åŒ–ä¸€å€‹åŸºæœ¬çš„åˆ†é¡æ¨¡å‹
                self.model = BertForSequenceClassification.from_pretrained(
                    settings.MODEL_NAME,
                    num_labels=settings.NUM_LABELS,
                    hidden_dropout_prob=settings.DROPOUT_PROB
                )
                self.model.eval()
            
            # è¼‰å…¥åƒè€ƒåµŒå…¥å‘é‡
            ref_embeddings_file = model_dir / settings.REFERENCE_EMBEDDINGS_FILE
            if ref_embeddings_file.exists():
                logger.info("è¼‰å…¥åƒè€ƒåµŒå…¥å‘é‡...")
                with open(ref_embeddings_file, "rb") as f:
                    self.reference_embeddings = pickle.load(f)
                logger.info("åƒè€ƒåµŒå…¥å‘é‡è¼‰å…¥æˆåŠŸ")
            else:
                logger.warning("åƒè€ƒåµŒå…¥å‘é‡æª”æ¡ˆä¸å­˜åœ¨ï¼Œå°‡åœ¨é¦–æ¬¡ä½¿ç”¨æ™‚ç”Ÿæˆ")
                # å¦‚æœæ²’æœ‰åƒè€ƒåµŒå…¥ï¼Œå˜—è©¦å¾åƒè€ƒè³‡æ–™ç”Ÿæˆ
                await self._generate_reference_embeddings()
            
            self._initialized = True
            logger.info("æ¨¡å‹æ¨è«–å™¨åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"æ¨¡å‹åˆå§‹åŒ–å¤±æ•—: {e}")
            raise
    
    async def _generate_reference_embeddings(self):
        """å¾åƒè€ƒè³‡æ–™ç”ŸæˆåµŒå…¥å‘é‡"""
        try:
            four_file = settings.REFERENCE_DIR / "attack_chain_FourInOne.json"
            apt_file = settings.REFERENCE_DIR / "attack_chain_APT29.json"
            
            if four_file.exists() and apt_file.exists():
                logger.info("å¾åƒè€ƒè³‡æ–™ç”ŸæˆåµŒå…¥å‘é‡...")
                
                # è¼‰å…¥åƒè€ƒè³‡æ–™
                from .preprocessor import DataPreprocessor
                preprocessor = DataPreprocessor()
                
                four_logs = preprocessor.load_logs(four_file)
                apt_logs = preprocessor.load_logs(apt_file)
                
                # æå–ç‰¹å¾µåºåˆ—
                four_seq = preprocessor.extract_features(four_logs)
                apt_seq = preprocessor.extract_features(apt_logs)
                
                # ç”ŸæˆåµŒå…¥å‘é‡
                four_embedding = self.get_embedding(four_seq)
                apt_embedding = self.get_embedding(apt_seq)
                
                # ä¿å­˜åµŒå…¥å‘é‡
                self.reference_embeddings = {
                    "four_embedding": four_embedding,
                    "apt_embedding": apt_embedding
                }
                
                ref_embeddings_file = settings.MODEL_DIR / settings.REFERENCE_EMBEDDINGS_FILE
                with open(ref_embeddings_file, "wb") as f:
                    pickle.dump(self.reference_embeddings, f)
                
                logger.info("åƒè€ƒåµŒå…¥å‘é‡ç”Ÿæˆä¸¦ä¿å­˜å®Œæˆ")
            else:
                logger.warning("åƒè€ƒè³‡æ–™æª”æ¡ˆä¸å­˜åœ¨ï¼Œè·³éåµŒå…¥å‘é‡ç”Ÿæˆ")
                self.reference_embeddings = None
                
        except Exception as e:
            logger.error(f"ç”Ÿæˆåƒè€ƒåµŒå…¥å‘é‡å¤±æ•—: {e}")
            self.reference_embeddings = None
    
    def is_ready(self) -> bool:
        """æª¢æŸ¥æ¨¡å‹æ˜¯å¦æº–å‚™å°±ç·’"""
        return (self._initialized and 
                self.model is not None and 
                self.tokenizer is not None and
                self.bert_model is not None)
    
    def extract_features(self, logs: List[Dict[str, Any]]) -> str:
        """å¾æ—¥èªŒä¸­æå–ç‰¹å¾µåºåˆ—"""
        sequences = []
        for log in logs:
            desc = log.get("rule.description", "")
            src_ip = log.get("data.srcip", "None")
            dst_ip = log.get("data.dstip", "None")
            sequence = f"{desc} from {src_ip} to {dst_ip}"
            sequences.append(sequence)
        
        full_sequence = " ".join(sequences)
        return full_sequence
    
    def get_embedding(self, sequence: str) -> np.ndarray:
        """ç²å–åºåˆ—çš„BERTåµŒå…¥å‘é‡"""
        inputs = self.tokenizer(
            sequence,
            padding="max_length",
            truncation=True,
            max_length=settings.MAX_LENGTH,
            return_tensors="pt"
        )
        
        with torch.no_grad():
            outputs = self.bert_model(**inputs)
            embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()
        
        return embedding
    
    def cosine_similarity(self, a: np.ndarray, b: np.ndarray) -> float:
        """è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦"""
        return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))
    
    def predict_attack_chain(self, sequence: str) -> Tuple[str, Dict[str, float], float]:
        """é æ¸¬æ”»æ“Šéˆé¡å‹"""
        if not self.reference_embeddings:
            # ä½¿ç”¨åˆ†é¡æ¨¡å‹é æ¸¬
            return self._predict_with_classifier(sequence)
        
        # ä½¿ç”¨ç›¸ä¼¼åº¦æ–¹æ³•é æ¸¬
        emb = self.get_embedding(sequence)
        four_sim = self.cosine_similarity(emb, self.reference_embeddings["four_embedding"])
        apt_sim = self.cosine_similarity(emb, self.reference_embeddings["apt_embedding"])
        
        max_sim = max(four_sim, apt_sim)
        
        if max_sim < 0.5:
            pred_chain = "Unknown"
        else:
            pred_chain = "FourInOne" if four_sim > apt_sim else "APT29"
        
        similarities = {"FourInOne": four_sim, "APT29": apt_sim}
        
        return pred_chain, similarities, max_sim
    
    def _predict_with_classifier(self, sequence: str) -> Tuple[str, Dict[str, float], float]:
        """ä½¿ç”¨åˆ†é¡æ¨¡å‹é€²è¡Œé æ¸¬"""
        inputs = self.tokenizer(
            sequence,
            padding="max_length",
            truncation=True,
            max_length=settings.MAX_LENGTH,
            return_tensors="pt"
        )
        
        with torch.no_grad():
            outputs = self.model(**inputs)
            probs = torch.softmax(outputs.logits, dim=1).numpy()[0]
            pred_label = np.argmax(probs)
        
        # æ¨™ç±¤æ˜ å°„
        label_map = {0: "FourInOne", 1: "APT29"}
        pred_chain = label_map.get(pred_label, "Unknown")
        confidence = float(probs[pred_label])
        
        similarities = {
            "FourInOne": float(probs[0]),
            "APT29": float(probs[1])
        }
        
        return pred_chain, similarities, confidence
    
    async def predict_logs(self, logs: List[Dict[str, Any]]) -> Dict[str, Any]:
        """å°æ—¥èªŒåˆ—è¡¨é€²è¡Œé æ¸¬"""
        if not self.is_ready():
            raise Exception("æ¨¡å‹å°šæœªåˆå§‹åŒ–")
        
        try:
            # æå–ç‰¹å¾µåºåˆ—
            sequence = self.extract_features(logs)
            
            # åŸ·è¡Œé æ¸¬
            prediction, similarities, confidence = self.predict_attack_chain(sequence)
            
            # å¦‚æœä¿¡å¿ƒåº¦å¾ˆé«˜ï¼Œç™¼é€å‘Šè­¦
            if confidence >= settings.MODEL_CONFIDENCE_THRESHOLD:
                await self._send_alert(logs, prediction, confidence, similarities)
            
            return {
                "prediction": prediction,
                "confidence": confidence,
                "similarities": similarities,
                "sequence_length": len(sequence),
                "logs_count": len(logs)
            }
            
        except Exception as e:
            logger.error(f"é æ¸¬å¤±æ•—: {e}")
            raise
    
    async def _send_alert(self, logs: List[Dict], prediction: str, confidence: float, similarities: Dict[str, float]):
        """ç™¼é€å‘Šè­¦åˆ°å¤–éƒ¨API"""
        try:
            # æº–å‚™å‘Šè­¦è³‡æ–™
            now = datetime.now()
            formatted_time = now.strftime("%Y-%m-%dT%H:%M:%S.%f")[:-3] + "+0000"
            
            # æ‰¾åˆ°æœ€é«˜ä¿¡å¿ƒåº¦çš„æ—¥èªŒä½œç‚ºä¸»è¦å‘Šè­¦
            if logs:
                main_log = logs[0]  # ç°¡åŒ–è™•ç†ï¼Œå–ç¬¬ä¸€å€‹
                
                # æº–å‚™å€‹åˆ¥å‘Šè­¦
                alerts = []
                for log in logs:
                    alert = {
                        "timestamp": log.get("timestamp", formatted_time),
                        "rule_id": log.get("rule.id", ""),
                        "rule_level": log.get("rule.level", ""),
                        "description": log.get("rule.description", ""),
                        "src_ip": log.get("data.srcip", "None"),
                        "dst_ip": log.get("data.dstip", "None"),
                        "full_log": log.get("full_log", "")
                    }
                    alerts.append(alert)
                
                # æº–å‚™é—œè¯å‘Šè­¦
                involved_info = [
                    f"Agent: {log.get('agent.id', 'Unknown')}, "
                    f"Src: {log.get('data.srcip', 'None')}, "
                    f"Dst: {log.get('data.dstip', 'None')}, "
                    f"Confidence: {confidence:.4f}"
                    for log in logs[:5]  # æœ€å¤šé¡¯ç¤º5å€‹
                ]
                
                correlation_alert = {
                    "timestamp": formatted_time,
                    "rule_id": 51110,
                    "rule_level": 12,
                    "description": f"æ”»æ“Šéˆæª¢æ¸¬: {prediction}",
                    "src_ip": main_log.get("data.srcip", "None"),
                    "dst_ip": main_log.get("data.dstip", "None"),
                    "full_log": f"æª¢æ¸¬åˆ° {prediction} æ”»æ“Šéˆï¼Œä¿¡å¿ƒåº¦: {confidence:.4f}ï¼Œæ¶‰åŠäº‹ä»¶: {'; '.join(involved_info)}"
                }
                
                # çµ„ç¹”APIè³‡æ–™
                api_data = {
                    "data": [{
                        "correlation_alert": correlation_alert,
                        "alerts": alerts
                    }]
                }
                
                # ç™¼é€åˆ°å‘Šè­¦API
                headers = {"Content-Type": "application/json"}
                async with aiohttp.ClientSession() as session:
                    async with session.post(
                        settings.ALERT_API_URL,
                        json=api_data,
                        headers=headers,
                        timeout=settings.ALERT_API_TIMEOUT
                    ) as response:
                        if response.status == 200:
                            logger.info(f"å‘Šè­¦ç™¼é€æˆåŠŸ: {prediction} (ä¿¡å¿ƒåº¦: {confidence:.4f})")
                        else:
                            logger.warning(f"å‘Šè­¦ç™¼é€å¤±æ•—ï¼Œç‹€æ…‹ç¢¼: {response.status}")
            
            # ç™¼é€Discordé€šçŸ¥ï¼ˆå¦‚æœé…ç½®äº†ï¼‰
            if settings.DISCORD_WEBHOOK_URL:
                await self._send_discord_notification(prediction, confidence, len(logs))
                
        except Exception as e:
            logger.error(f"ç™¼é€å‘Šè­¦å¤±æ•—: {e}")
    
    async def _send_discord_notification(self, prediction: str, confidence: float, logs_count: int):
        """ç™¼é€Discordé€šçŸ¥"""
        try:
            import aiohttp
            
            embed = {
                "title": "ğŸš¨ æ”»æ“Šéˆæª¢æ¸¬å‘Šè­¦",
                "description": f"æª¢æ¸¬åˆ° **{prediction}** æ”»æ“Šéˆ",
                "color": 0xff0000 if confidence > 0.7 else 0xff9900,
                "fields": [
                    {"name": "ä¿¡å¿ƒåº¦", "value": f"{confidence:.4f}", "inline": True},
                    {"name": "äº‹ä»¶æ•¸é‡", "value": str(logs_count), "inline": True},
                    {"name": "æ™‚é–“", "value": datetime.now().strftime("%Y-%m-%d %H:%M:%S"), "inline": True}
                ],
                "footer": {"text": "æ”»æ“Šéˆæª¢æ¸¬ç³»çµ±"}
            }
            
            webhook_data = {"embeds": [embed]}
            
            async with aiohttp.ClientSession() as session:
                async with session.post(settings.DISCORD_WEBHOOK_URL, json=webhook_data) as response:
                    if response.status == 204:
                        logger.info("Discordé€šçŸ¥ç™¼é€æˆåŠŸ")
                    else:
                        logger.warning(f"Discordé€šçŸ¥ç™¼é€å¤±æ•—ï¼Œç‹€æ…‹ç¢¼: {response.status}")
                        
        except Exception as e:
            logger.error(f"Discordé€šçŸ¥ç™¼é€å¤±æ•—: {e}")
    
    async def get_status(self) -> Dict[str, Any]:
        """ç²å–æ¨¡å‹ç‹€æ…‹è³‡è¨Š"""
        try:
            model_config_file = settings.MODEL_DIR / settings.MODEL_CONFIG_FILE
            
            if model_config_file.exists():
                with open(model_config_file, 'r', encoding='utf-8') as f:
                    config = json.load(f)
            else:
                config = {}
            
            return {
                "initialized": self._initialized,
                "model_ready": self.is_ready(),
                "model_version": config.get("version", "unknown"),
                "model_timestamp": config.get("timestamp", "unknown"),
                "confidence_threshold": settings.MODEL_CONFIDENCE_THRESHOLD,
                "last_check": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"ç²å–æ¨¡å‹ç‹€æ…‹å¤±æ•—: {e}")
            return {
                "initialized": False,
                "model_ready": False,
                "error": str(e)
            }
